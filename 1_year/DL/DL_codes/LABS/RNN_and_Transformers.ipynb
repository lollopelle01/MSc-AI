{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2mmcVtW1NUo"
      },
      "source": [
        "# RNN and Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVw8qZ4H1KsP"
      },
      "source": [
        "In this lab lesson we will see how and when to use Recurrent Neural Networks and how to exploits pre-trained Transformers model like Bert.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f4UeT2u2vgq"
      },
      "source": [
        "## Task description\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAUzFjP_3YAa"
      },
      "source": [
        "In this exercise we will try to classify subjectivity of text in sentences.\n",
        "\n",
        "We will use a collection of 103 Italian newspaper's articles labeled as Objective or Subjective. Each article is divided in sentences, which are consequently classified as either Subjective or Objective.\n",
        "\n",
        "You can find the data along with a more detailed description [here](https://github.com/francescoantici/SubjectivITA).\n",
        "\n",
        "We will be trying to create a model which is able to predict if a sentence contains subjectivity or it is fully objective.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hJ8KsB215Kp"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QypPJoE1ylP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTi3RT8Z3uON"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtmNGBY64aKX"
      },
      "source": [
        "Please download the train, val and test files from the [repository](https://github.com/francescoantici/SubjectivITA/tree/main/datasets/sentences).\n",
        "\n",
        "After having uploaded the three files to the notebook, use this utility function to load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvByVSDP4ln7"
      },
      "outputs": [],
      "source": [
        "def get_data_split(split):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    - split: the split of the data you want to load.\n",
        "  Returns:\n",
        "    - X, y data, where X is the array containing the sentences and y is the labels vector.\n",
        "\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(f\"sentences{split.capitalize()}.csv\")\n",
        "  return df['FRASE'].values, df['TAG_FRASE'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdNzDM2W5PkG"
      },
      "outputs": [],
      "source": [
        "sentences_train, labels_train = get_data_split(split = 'train')\n",
        "sentences_val, labels_val = get_data_split(split = 'val')\n",
        "sentences_test, labels_test = get_data_split(split = 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upAynpZE5grI"
      },
      "source": [
        "### Data Inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7-hGV0u5qux"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywLGSfth7wrb"
      },
      "source": [
        "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
        "modeling sequence data such as time series or natural language.\n",
        "\n",
        "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
        "sequence, while maintaining an internal state that encodes information about the\n",
        "timesteps it has seen so far.\n",
        "\n",
        "The Keras RNN API is designed with a focus on:\n",
        "\n",
        "- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n",
        "`keras.layers.GRU` layers enable you to quickly build recurrent models without\n",
        "having to make difficult configuration choices.\n",
        "\n",
        "- **Ease of customization**: You can also define your own RNN cell layer (the inner\n",
        "part of the `for` loop) with custom behavior, and use it with the generic\n",
        "`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n",
        "prototype different research ideas in a flexible way with minimal code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cXxjtEM765h"
      },
      "source": [
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "1. `keras.layers.SimpleRNN`, a fully-connected RNN where the output from previous\n",
        "timestep is to be fed to next timestep.\n",
        "\n",
        "2. `keras.layers.GRU`, first proposed in\n",
        "[Cho et al., 2014](https://arxiv.org/abs/1406.1078).\n",
        "\n",
        "3. `keras.layers.LSTM`, first proposed in\n",
        "[Hochreiter & Schmidhuber, 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Xg_LD_79uT"
      },
      "source": [
        "### Data pre-processsing\n",
        "\n",
        "We will use a tokenizer [function](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) provided by Keras to map each token to an integer, so that the model is able to interpreter it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL-CjtIr782R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "lbl_to_idx_dict = {\"OGG\":0, \"SOG\":1}\n",
        "\n",
        "label_to_idx_f = np.vectorize(lbl_to_idx_dict.get)\n",
        "\n",
        "vocabulary_dim = 10000\n",
        "\n",
        "def get_tokenizer(x_train):\n",
        "  tokenizer = Tokenizer(num_words = vocabulary_dim)\n",
        "  tokenizer.fit_on_texts(x_train)\n",
        "  return tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(sentences_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzt_qt9rbk-N"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "maxSentenceLen = 20\n",
        "\n",
        "generate_x = lambda x: pad_sequences(tokenizer.texts_to_sequences(x), maxlen = maxSentenceLen, padding = \"post\")\n",
        "\n",
        "x_train = generate_x(sentences_train)\n",
        "x_test = generate_x(sentences_test)\n",
        "x_val = generate_x(sentences_val)\n",
        "\n",
        "y_train = label_to_idx_f(labels_train)\n",
        "y_test = label_to_idx_f(labels_test)\n",
        "y_val = label_to_idx_f(labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUmZqKiic6Zh"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIUdd95k8gop"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xokrUCPGqfvZ"
      },
      "source": [
        "let's build a rnn baseline based on LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGB2xMC18iwW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def get_rnn_model(input_shape, out_dim, vocabulary_dim):\n",
        "  input = Input(shape=input_shape)\n",
        "\n",
        "  embedding_layer = Embedding(input_dim=vocabulary_dim, output_dim=64)(input)\n",
        "\n",
        "  lstm_1 = LSTM(128, return_sequences=True, recurrent_dropout = 0.2)(embedding_layer)\n",
        "\n",
        "  lstm_2 = LSTM(64, dropout = 0.2)(lstm_1)\n",
        "\n",
        "  output = Dense(out_dim)(lstm_2)\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = Adam(1e-3), metrics = ['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTbbDv8KzqKq"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 5, restore_best_weights = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwXMid6T_n5X"
      },
      "outputs": [],
      "source": [
        "model = get_rnn_model((20,), 2, vocabulary_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtnrwygRcmTt"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=10, validation_data = (x_val, y_val), callbacks = [callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SlpWhGHCiYc"
      },
      "source": [
        "### Model evaluation\n",
        "\n",
        "For the model evaluation we will use the [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function provided by scikit-learn, which will present a detailed report of the performances of the model evaluated on different metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYBXj27zCkkm"
      },
      "outputs": [],
      "source": [
        "toLabels = np.vectorize(lambda e: \"OGG\" if e == 0 else \"SOG\")\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    - model: the model to use to make the prediction.\n",
        "    - x_test: the sentences to label.\n",
        "    - y_test: the actual labels.\n",
        "  Returns:\n",
        "    - The results of the evaluation of the model.\n",
        "\n",
        "  \"\"\"\n",
        "  y_pred = np.argmax(model.predict(x_test), axis = -1)\n",
        "  print(classification_report(toLabels(y_test), y_pred = toLabels(y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5UA2u5ADfgV"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbLopRmb-CN0"
      },
      "source": [
        "#### Bidirectional RNNs\n",
        "\n",
        "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
        "can perform better if it not only processes sequence from start to end, but also\n",
        "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
        "have the context around the word, not only just the words that come before it.\n",
        "\n",
        "Keras provides an easy API for you to build such **bidirectional RNNs**: the\n",
        "`keras.layers.Bidirectional` wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NxMWVoU9971"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
        "\n",
        "def get_rnn_model_bd(input_shape, out_dim, vocabulary_length):\n",
        "  input = Input(shape=input_shape)\n",
        "\n",
        "  embedding_layer = Embedding(input_dim=vocabulary_length, output_dim=64)(input)\n",
        "\n",
        "  lstm_1 = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout = 0.2))(embedding_layer)\n",
        "\n",
        "  lstm_2 = Bidirectional(LSTM(64, dropout = 0.2))(lstm_1)\n",
        "\n",
        "  output = Dense(out_dim)(lstm_2)\n",
        "\n",
        "  model = Model(input, output)\n",
        "\n",
        "  model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = Adam(1e-3), metrics = ['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB-tTuuKATXW"
      },
      "outputs": [],
      "source": [
        "model_bd = get_rnn_model_bd((20,), 2, vocabulary_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odtIo3d6jdC2"
      },
      "outputs": [],
      "source": [
        "history = model_bd.fit(x_train, y_train, epochs=10, validation_data = (x_val, y_val), callbacks = [callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEfvrHfRjgtt"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model_bd, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9e0VvSD52rV"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U_P9h2t59Bw"
      },
      "source": [
        "Transformers are deep neural networks that over the last years achieved state of the art performances in several tasks.\n",
        "\n",
        "Transformers replaces CNNs and RNNs with [self-attention](https://developers.google.com/machine-learning/glossary#self-attention). Self attention allows Transformers to easily transmit information across the input sequences.\n",
        "\n",
        "For the transformer model implementation we will rely on a Python library called `transformers`, which provides an API inteface to several pre-trained models for fine-tuning or transfer-learning purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVjERW866TJp"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers \n",
        "from transformers import AutoTokenizer, TFBertModel"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YQfr8j8OtqEy"
      },
      "source": [
        "### Pre-trained model\n",
        "\n",
        "For this task we will use a pre-trained language model called [AlBERTo](github.com/marcopoli/AlBERTo-it). AlBERTo is a BERT model trained for the Italian language. In particular, AlBERTo is focused on the language used in social networks, specifically on Twitter. Due to the language and the type of data present in the dataset AlBERTo is the best fit for this kind of task.\n",
        "\n",
        "You can find the pre-trained model in [huggingface](https://huggingface.co/bert-base-multilingual-cased?text=mi+piace+il+%5BMASK%5D), which is an open repository for pre-trained architectures, available in both pytorch and tensorflow (depending on the developers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM0ZfujT6-J2"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r__5kvn8BPle"
      },
      "source": [
        "Transoformes must recieve input in a standard format, namely divided in `input_ids`, `token_type_ids`, `attention_mask`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHR30lup67iO"
      },
      "outputs": [],
      "source": [
        "def prepare_data_bert(x, y, maxSentenceLen = maxSentenceLen):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    - x: the sentences to label.\n",
        "    - y: the actual labels.\n",
        "    - maxSentenceLen: The maximum length of the sentences, it is used as a truncation length\n",
        "  Returns:\n",
        "    - A tuple with the input to feed into a transformers model, namely ((input_ids, attention_mask, token_type_ids), categorical_labels).\n",
        "\n",
        "  \"\"\"\n",
        "  pad = tf.keras.preprocessing.sequence.pad_sequences\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
        "  dataFields = {\n",
        "          \"input_ids\": [],\n",
        "          \"token_type_ids\": [],\n",
        "          \"attention_mask\": [],\n",
        "          \"subjectivity\": []\n",
        "      }\n",
        "  lbls = {\n",
        "      'SOG' : 1.0,\n",
        "      'OGG' : 0.0\n",
        "  }\n",
        "  for i in range(len(x)):\n",
        "      data = tokenizer(x[i])\n",
        "      padded = pad([data['input_ids'], data['attention_mask'], data['token_type_ids']], padding = 'post', maxlen = maxSentenceLen)\n",
        "      dataFields['input_ids'].append(padded[0])\n",
        "      dataFields['attention_mask'].append(padded[1])\n",
        "      dataFields['token_type_ids'].append(padded[-1])\n",
        "      dataFields['subjectivity'].append(lbls[y[i]])\n",
        "  \n",
        "  for key in dataFields:\n",
        "      dataFields[key] = np.array(dataFields[key])\n",
        "  \n",
        "  return [dataFields[\"input_ids\"], dataFields[\"token_type_ids\"], dataFields[\"attention_mask\"]], dataFields[\"subjectivity\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oDhftmAgoj"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcz-g_bfAf-K"
      },
      "outputs": [],
      "source": [
        "def create_transformers_model(input_shape, out_dim):\n",
        "\n",
        "    input_ids = Input(shape=input_shape, dtype=tf.int32)\n",
        "    token_type_ids = Input(shape=input_shape, dtype=tf.int32)\n",
        "    attention_mask = Input(shape=input_shape, dtype=tf.int32)\n",
        "\n",
        "    bertModel = TFBertModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[-1]\n",
        "\n",
        "    out = Dense(out_dim, activation=tf.nn.sigmoid)(tf.keras.layers.Dropout(0.1)(bertModel))\n",
        "\n",
        "    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n",
        "\n",
        "    model.compile(optimizer = Adam(1e-5), loss = SparseCategoricalCrossentropy(from_logits=True), metrics = ['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hByBS2IRkibb"
      },
      "outputs": [],
      "source": [
        "bert_model = create_transformers_model((20,), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBFNY98JuNv0"
      },
      "outputs": [],
      "source": [
        "x_train_bert, y_train_bert = prepare_data_bert(sentences_train, labels_train)\n",
        "x_val_bert, y_val_bert = prepare_data_bert(sentences_val, labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukHU9hO7k3Ut"
      },
      "outputs": [],
      "source": [
        "history = bert_model.fit(x_train_bert, \n",
        "                         y_train_bert, \n",
        "                         epochs=4, \n",
        "                         validation_data = (x_val_bert, y_val_bert), \n",
        "                         batch_size = 16, \n",
        "                         callbacks = [callback]\n",
        "                         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prSfUspMDOFR"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VZGxpniDmRc"
      },
      "outputs": [],
      "source": [
        "x_test_bert, y_test_bert = prepare_data_bert(sentences_test, labels_test)\n",
        "\n",
        "evaluate_model(bert_model, x_test_bert, y_test_bert)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "H9e0VvSD52rV"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
