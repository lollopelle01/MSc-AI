{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "random_state = 42  # set in order to guarantee repetability of results\n",
    "data_url1 = 'exam_superv.csv'\n",
    "sep1 = ','\n",
    "np.random.seed(random_state)\n",
    "\n",
    "\n",
    "# Supervised data with attributes also\n",
    "# add index_col = 0 if not intereste in having the index column\n",
    "# add index_col = 'index' to name the column index with \"Index\"\n",
    "df = pd.read_csv(data_url1, sep=sep1)\n",
    "\n",
    "# Supervised data with attributes also\n",
    "# To replace all the occurrencies of the value '?' with a Nan value\n",
    "df = pd.read_csv(data_url1, sep=sep1, na_values=['?'])\n",
    "\n",
    "# Not supervised data with attributes\n",
    "df = pd.read_csv(data_url1, sep=sep1, header=None)\n",
    "\n",
    "# Not supervised with attributes but assigned names\n",
    "df = pd.read_csv(data_url1, sep=sep1, header=None\\\n",
    "            , names=['sepal length', 'sepal width', 'petal length'])\n",
    "#Reading xlsx files\n",
    "df = pd.read_excel('FoodUK2014.xlsx')\n",
    "\n",
    "#Unsupervised without attributes\n",
    "df = np.loadtxt('data_file', delimiter = sep1)\n",
    "X = pd.DataFrame(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=...\n",
    "#To drop null values and store modification\n",
    "df.shape\n",
    "df = df.dropna()\n",
    "\n",
    "#To detect the rows that are null (it creates a mask)\n",
    "df.isna()\n",
    "\n",
    "#Showing the number of null for each remaining column\n",
    "df.isna().sum()\n",
    "\n",
    "#To have all the rows where a certain target is NaN\n",
    "df[df[target].isna()]\n",
    "\n",
    "#To have all the rows where there's at least one Nan\n",
    "nan_rows = df.isna().any(axis=1)\n",
    "df[nan_rows]\n",
    "\n",
    "#If we want to treat a certain column (that is marked as object) like a string and apply some string conditions on\n",
    "df[target].astype('str').str.contains('C')\n",
    "\n",
    "#take just those rows that respect the condition string on the element\n",
    "df[df[target].astype('str').str.contains('C')]\n",
    "\n",
    "#take just those rows that don't respect the condition string on the element\n",
    "df[~df[target].astype('str').str.contains('C')]\n",
    "#Or\n",
    "df.drop(df[df[target].astype('str').str.contains('C')].index, axis=0)\n",
    "\n",
    "#Shows the number of na in each remaining column\n",
    "df.isna().sum()\n",
    "\n",
    "#Get a desciption\n",
    "df.describe()\n",
    "\n",
    "#Get a description of the categorical attributes\n",
    "df0.describe(include= \"O\")\n",
    "\n",
    "#If we want to have a description also of non numeric data we need to explicitly say\n",
    "cat_attributes = df.dtypes.loc[df.dtypes=='object'].index.values \n",
    "df[cat_attributes].describe()\n",
    "\n",
    "\n",
    "#to check all the types of the data\n",
    "df.dtypes\n",
    "\n",
    "#To drop columns\n",
    "df_num = df.drop(['column'],axis=1) # axis= 1 drops the column\n",
    "\n",
    "#To drop the rows where there is a NaN value in the target columns\n",
    "df = df.dropna(axis=0, subset=[target])\n",
    "\n",
    "#To drop certain rows by index\n",
    "df_num = df.drop(df_num.index[5],axis=0) # axis= 1 drops the column\n",
    "\n",
    "#to drop certain rows by index after filtrating the dataset\n",
    "df.drop(df[df[target] == 'something'].index, axis=0)\n",
    "\n",
    "#to have the frequencies for each unique value \n",
    "np.unique(X, return_counts = True)\n",
    "\n",
    "# To have frequencies for each unique value with dataframe\n",
    "df[target].value_counts()\n",
    "\n",
    "#Return all the unique values and frequencies\n",
    "clust_sizes_km = np.unique(y_km,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram\n",
    "pd.DataFrame.hist(df,figsize = [10,10]);\n",
    "plt.show()\n",
    "\n",
    "#Histogram of one attribute, useful to show frequencies of categorical data\n",
    "plt.hist(df['quality'])\n",
    "plt.show()\n",
    "\n",
    "#Pairplot with the respect of a target value\n",
    "sns.pairplot(df, hue = 'quality')\n",
    "\n",
    "#Pairplot in general\n",
    "sns.pairplot(df)\n",
    "\n",
    "#Correlation matrix- useful for feature selection to check correlation of features\n",
    "#The higer the correlation is among features means that we can eliminate one of them because they have similar trends\n",
    "#But if we have to evaluate the best feature to perform a clustering with the target then the higer it is the better\n",
    "corr = df[df.columns].corr()\n",
    "sns.heatmap(corr,  annot_kws={\"size\": 6},square = True,cmap=\"YlGnBu\", annot=True, cbar_kws={'aspect': 100})\n",
    "plt.show()\n",
    "\n",
    "#--BoxPlot of every attribute- useful for outliers\n",
    "plt.figure(figsize=(15,15))\n",
    "pos = 1\n",
    "for i in df.columns:\n",
    "    plt.subplot(3, 4, pos)\n",
    "    sns.boxplot(df[i])\n",
    "    plt.title(i)\n",
    "    pos += 1\n",
    "\n",
    "#BoxPlot of everything in one plot\n",
    "sns.boxplot(data = df, width=1,linewidth=1)\n",
    "\n",
    "#BoxPlot comparing 2 attributes\n",
    "sns.boxplot(x='quality', y='fixed acidity', data = df)\n",
    "\n",
    "\n",
    "#Scatterplot of interesting columns of the pairplot\n",
    "interesting_columns = [0,1]\n",
    "plt.scatter(X.iloc[:,interesting_columns[0]], X.iloc[:,interesting_columns[1]],\n",
    "              c = y #if I have the set of labels\n",
    "              #or if I want just a color\n",
    "            #, c='white'          # color filling the data markers\n",
    "            , edgecolors='black' # edge color for data markers\n",
    "            , marker='o'         # data marker shape, e.g. triangles (v<>^), square (s), star (*), ...\n",
    "            , s=50)              # data marker size\n",
    "plt.grid()  \n",
    "plt.show()\n",
    "\n",
    "#OR\n",
    "#Remember to change chosen_dim with the name of the labels instead of [0,1] if the features are string\n",
    "chosen_dim = [0,1] #OR chosen_dim = ['label1', 'label2']\n",
    "X=...\n",
    "y = ...\n",
    "sns.scatterplot(x = chosen_dim[0],y = chosen_dim[1],data=X, hue = y)\n",
    "\n",
    "#To compare y and y_pred in the scatterPlot \n",
    "y_pred = ...\n",
    "sns.scatterplot(x = interesting_columns[0],y = interesting_columns[1], data=X, hue = y, style=(y == y_pred))\n",
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "estimator=...\n",
    "X_test=...\n",
    "y_test=...\n",
    "cm = confusion_matrix(y_test, y_pred, labels=estimator.classes_,\n",
    "    #normalize = 'true :  if 'true', the confusion matrix is normalized over the true conditions (e.g. rows); (sensitivity)\n",
    "    #normalize = 'pred': if 'pred', the confusion matrix is normalized over the predicted conditions (e.g. columns); (precision)\n",
    "    #normalize = 'all' : the confusion matrix is normalized by the total number of samples; (frequency)\n",
    ")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=estimator.classes_)\n",
    "disp.plot()\n",
    "\n",
    "#or\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "print(conf)\n",
    "print()\n",
    "print(conf/conf.sum()) #notice that computing this is the same of doing the plot_confusion_matrix with normalize='all'\n",
    "print(\"The percentage of match between the two clustering schemes is {:6.2f}%\"\\\n",
    "    .format((conf / conf.sum()).diagonal().sum()*100))\n",
    "\n",
    "#Plot all the attributes with the respect to a target class\n",
    "ncols=3\n",
    "import math\n",
    "nrows = math.ceil((df.shape[1]-1)/ncols)\n",
    "figwidth = ncols * 7\n",
    "figheigth = nrows*5\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheigth),sharey=True)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "fig.suptitle(\"Predicting variables versus target\", fontsize=18, y=0.95)\n",
    "\n",
    "for c, ax in zip(df.drop(target,axis=1).columns,axs.ravel()):\n",
    "    df.sort_values(by=c).plot.scatter(x=c,y=target\n",
    "                                    , title = '\"{}\" versus \"{}\"'.format(target,c)\n",
    "                                    , ax=ax)\n",
    "\n",
    "#Train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
    "                                                    , train_size = 0.66\n",
    "                                                    , random_state = random_state) # default Train 0.75- Test 0.25\n",
    "print(\"There are {} samples in the training dataset\".format(X_train.shape[0]))\n",
    "print(\"There are {} samples in the testing dataset\".format(X_test.shape[0]))\n",
    "print(\"Each sample has {} features\".format(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From ordinal/categorical to numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the **ordinal transformer** generates a mapping from strings to numbers according to the lexicographic sorting of the strings; in this particular case, the strings indicate numeric subranges, and ranges with one digit constitute exceptions\n",
    "        '5-9' happens to be after '20-25'\n",
    "    - it is necessary to transform '5-9' into '05-09', and the same for other similar cases\n",
    "    - a way to do this is to prepare dictionaries for the translation and use the `.map` function\n",
    "    \n",
    "Vedi lab 04 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m non_numeric_features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mloc[df\u001b[38;5;241m.\u001b[39mdtypes \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe non-numeric features are:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(non_numeric_features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "non_numeric_features = df.dtypes.loc[df.dtypes == 'object'].index.values\n",
    "print(\"The non-numeric features are:\")\n",
    "print(non_numeric_features)\n",
    "\n",
    "numeric_features = list(set(df.dtypes.index.values)-set(non_numeric_features))\n",
    "print(\"The numeric features are:\")\n",
    "print(numeric_features)\n",
    "\n",
    "ordinal_features =['age', 'tumor-size','inv-nodes'] #This is an example, ordinal features are chosen by hand\n",
    "print(\"The ordinal features are:\")\n",
    "print(ordinal_features)\n",
    "\n",
    "categorical_features = list(set(non_numeric_features) - set(ordinal_features) - set(['Class']))\n",
    "print(\"The categorical features are:\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If I have to transform only ordinal to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "#Ordinal Encoder passing a list of non_numerical_features\n",
    "non_numeric_features = df.dtypes.loc[df.dtypes == 'object'].index.values\n",
    "transf_dtype = np.int32 # type to be used when converting\n",
    "ordinal_transformer = OrdinalEncoder(dtype = transf_dtype) # we assume the values are encoded so that lexicographic order = intended order\n",
    "df_2 = df.copy()\n",
    "df_2[non_numeric_features] = ordinal_transformer.fit_transform(df[non_numeric_features])\n",
    "df_2.head() #just to check\n",
    "\n",
    "#Ordinal Encoder passing a list of index of non_numerical_features\n",
    "non_numeric_features = [2]\n",
    "transf_dtype = np.int32 # type to be used when converting\n",
    "ordinal_transformer = OrdinalEncoder(dtype = transf_dtype) # we assume the values are encoded so that lexicographic order = intended order\n",
    "df_2 = df.copy()\n",
    "df_2.iloc[:,non_numeric_features] = ordinal_transformer.fit_transform(df.iloc[:,non_numeric_features])\n",
    "df_2.head() #just to check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If I have to transform both ordinal and categorical to numeric at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#It generates a column for each different values present in the categorical feature that you want to transform\n",
    "transf_dtype = np.int32\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse = False, dtype = transf_dtype)\n",
    "encoded = ohe.fit_transform(df[[target]])\n",
    "encoded_df = pd.DataFrame(encoded, columns=df[target].unique())\n",
    "df = df.drop(target, axis= 1)\n",
    "df[encoded_df.columns] = encoded_df\n",
    "\n",
    "\n",
    "#Ordinal Encoder\n",
    "ordinal_transformer = OrdinalEncoder(dtype = transf_dtype)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [('cat', categorical_transformer, categorical_features),\n",
    "                    ('ord', ordinal_transformer, ordinal_features)\n",
    "                   ],\n",
    "                    remainder = 'passthrough'\n",
    "    )\n",
    "\n",
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']\n",
    "\n",
    "preprocessor.fit(X)\n",
    "print(preprocessor.named_transformers_)\n",
    "\n",
    "X_p = preprocessor.fit_transform(X)\n",
    "df_p = pd.DataFrame(X_p) #just to check\n",
    "df_p.head()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_p,y, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "To detect the most promising features for clustering in a pairplot, you can look for features that have clear separation between different clusters or classes. You can also look for features that exhibit non-linear relationships and high density concentration, as these may indicate that they could be useful for creating clusters. Additionally, you can also perform dimensionality reduction techniques, such as PCA or t-SNE, to visualize the data in a lower dimensional space and identify which features are most important for clustering.\n",
    "You can also llok for isotropic distributions An isotropic cluster is a cluster in which the data points are distributed uniformly and symmetrically in all directions from the center of the cluster. In other words, in an isotropic cluster, the variance of the data points is the same in all directions, and the shape of the cluster is roughly spherical or circular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from functools import partial\n",
    "\n",
    "# k = num, num is the number of the best subesets of features that will be selected (e.g. if I start from 4 features k must be <4)\n",
    "#KBEST\n",
    "kbest = SelectKBest(score_func=partial(mutual_info_classif,random_state=random_state), k=num) \n",
    "\n",
    "fit = kbest.fit(X,y)\n",
    "X_reducted = fit.transform(X)\n",
    "X_reducted.shape\n",
    "\n",
    "#RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "# feature extraction\n",
    "rfe = RFE(estimator, n_features_to_select=5)\n",
    "fit = rfe.fit(X, y)\n",
    "X_reducted = fit.transform(X)\n",
    "print(\"Feature Ranking: %s\", fit.ranking_)\n",
    "\n",
    "\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "print(\"Explained Variance:\", fit.explained_variance_ratio_)\n",
    "X_reducted = fit.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Trasformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is Skewed, which means that is concentrated just in one section of the plot, or there are frequent outliers (mostly visible in boxPlots) means that we have to apply a trasformation which can be both rescaling and mapping values in another range in order to show hidden features that cannot be properly spotted. \n",
    "Clustering is more effective in absence of outliers and with all the variables distributed in similar ranges.\n",
    "If the boxplots are small in range better rescaling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler,Normalizer\n",
    "#To perform minMax Scaler in the range [0,1]\n",
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "mms.fit_transform(X)\n",
    "\n",
    "#Normalizer\n",
    "nrm = Normalizer()\n",
    "nrm.fit_transform(X)\n",
    "\n",
    "#Rescaling\n",
    "#Square root sqrt\n",
    "X_sqrt = pd.concat([X.iloc[:,:2],X.iloc[:,2:].applymap(sqrt)],axis=1)\n",
    "\n",
    "#Apply log to all the columns that dont have negative values\n",
    "from math import log\n",
    "for column in X.columns:\n",
    "\n",
    "    # We don't want to transform columns with values\n",
    "    # Lower than or equal to zero\n",
    "    if len(X[column]) != sum(np.greater(X[column], 0)):\n",
    "        continue\n",
    "    X[column] = np.log(X[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "#Several common classifiers and parameters\n",
    "model_lbls = [\n",
    "              'dt', \n",
    "              'nb', \n",
    "              'lp', \n",
    "              'svc', \n",
    "             'knn',\n",
    "             'adb',\n",
    "             'rf',\n",
    "            ]\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_param_dt = [{'max_depth': [*range(1,20)], 'criterion':['gini', 'entropy'], 'min_samples_split': range(2,10)}]\n",
    "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
    "tuned_param_lp = [{'early_stopping': [True]}]\n",
    "tuned_param_svc = [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100, 1000],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100, 1000],                     \n",
    "                    },\n",
    "                   ]\n",
    "tuned_param_knn =[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "tuned_param_adb = [{'n_estimators':[20,30,40,50],\n",
    "                   'learning_rate':[0.5,0.75,1,1.25,1.5]}]\n",
    "tuned_param_rf = [{'max_depth': [*range(5,15)],\n",
    "                   'n_estimators':[*range(10,100,10)]}]\n",
    "\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(), \n",
    "           'param': tuned_param_dt,\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': tuned_param_nb\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(),\n",
    "           'param': tuned_param_lp,\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(), \n",
    "           'param': tuned_param_svc\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': tuned_param_knn\n",
    "       },\n",
    "       'adb':{'name': 'AdaBoost           ',\n",
    "           'estimator': AdaBoostClassifier(),\n",
    "           'param': tuned_param_adb\n",
    "          },\n",
    "    'rf': {'name': 'Random forest       ',\n",
    "           'estimator': RandomForestClassifier(),\n",
    "           'param': tuned_param_rf\n",
    "          }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Classifier by Tuning HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define just one type of classifier: decision tree in this case\n",
    "#change the params and estimator with the choices listed in the previous cell\n",
    "#gridSearchCv uses CrossValidation\n",
    "model_param = {'criterion':['gini', 'entropy'], 'max_depth':list(range(1,10)), 'min_samples_split': range(2,10)}\n",
    "estimator = tree.DecisionTreeClassifier()\n",
    "score = 'accuracy'\n",
    "\n",
    "\n",
    "clf = GridSearchCV(estimator, model_param, cv=5,\n",
    "                   scoring=score,\n",
    "                   return_train_score=False,\n",
    "                   n_jobs=2,  # this allows using multi-cores\n",
    "                   )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"best parameter estimator :\", clf.best_params_)\n",
    "print(\"best score for accuracy: {:.2f}%\".format(clf.best_score_*100))\n",
    "print()\n",
    "\n",
    "#Then predict the labels\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "#Evaluate accuracy of the model on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"the accuracy score is: \",round(accuracy_score(y_test,y_pred)*100, 2),'%' )\n",
    "\n",
    "\n",
    "#ALTERNATIVE to GridSearch is doing by hand the CrossValidation\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import cross_val_score\n",
    "parameter_values = tuned_param_dt\n",
    "pg = list(ParameterGrid(parameter_values))\n",
    "score = 'accuracy'\n",
    "avg_scores = []\n",
    "for i in range(len(pg)):\n",
    "    estimator = tree.DecisionTreeClassifier(**(pg[i])\n",
    "                                            , random_state = random_state\n",
    "                                            )\n",
    "    #cross validate split in cv = 5 subsets\n",
    "    scores = cross_val_score(estimator, X_train, y_train\n",
    "                             , scoring=score, cv = 5)\n",
    "    # cross_val_score produces an array with one score for each fold\n",
    "    avg_scores.append(np.mean(scores))\n",
    "#Find the best score\n",
    "print(avg_scores)\n",
    "best_parameter = pg[np.argmax(avg_scores)]\n",
    "best_score = max(avg_scores)\n",
    "#recreate the estimator\n",
    "estimator = tree.DecisionTreeClassifier(**(best_parameter))\n",
    "estimator.fit(X_train,y_train)\n",
    "y_predicted = estimator.predict(X_test)\n",
    "accuracy_cv = accuracy_score(y_test, y_predicted) * 100\n",
    "print(\"The accuracy on test set tuned with cross_validation is {:.1f}% with depth {}\".format(accuracy_cv, best_parameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print metrics and performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the best param of a model\n",
    "from sklearn.metrics import classification_report\n",
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    index = model.best_index_\n",
    "    mean = model.cv_results_['mean_test_score'][index]\n",
    "    std = model.cv_results_['std_test_score'][index]\n",
    "    param = model.cv_results_['params']\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2,model.best_params_ ))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all parameters of a model\n",
    "from sklearn.metrics import classification_report\n",
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing multiple classifiers to find the best one and the best combination of parameters for that classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test all classifiers maximizing 'precision' and 'recall'\n",
    "scores = ['precision', 'recall']\n",
    "results_short = {}\n",
    "for score in scores:\n",
    "    print('='*40)\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    #'%s_macro' % score ## is a string formatting expression\n",
    "    # the parameter after % is substituted in the string placeholder %s\n",
    "    for m in model_lbls:\n",
    "        print('-'*40)\n",
    "        print(\"Trying model {}\".format(models[m]['name']))\n",
    "        \n",
    "        #Super powerful, basically the function test the model for different values of the parameter that it takes as input (\"models[m]['param']\" in this case)\n",
    "        #and it gives us the best estimator that maximize a score that we pass as input (\"precision_macro\" in this case).\n",
    "        #It makes it by looping cross_validation procedures that have cv = 5.\n",
    "        #It's basically what we have done in the previous lab when we've iteratred through all the possible values for a certain parameter and instanciated\n",
    "        #every time a new estimator with the new parameter_value and evaluating the score using cross_validation. All this process is now made just by calling this\n",
    "        #function once.\n",
    "        clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=5,\n",
    "                           scoring='%s_macro' % score, \n",
    "                           return_train_score = False,\n",
    "                           n_jobs = 2, # this allows using multi-cores\n",
    "                           )\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        print_results(clf)\n",
    "        results_short[m] = clf.best_score_\n",
    "    print(\"Summary of results for {}\".format(score))\n",
    "    print(\"Estimator\")\n",
    "    for m in results_short.keys():\n",
    "        print(\"{}\\t - score: {:4.2}%\".format(models[m]['name'], results_short[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_columns = [0 , 1]\n",
    "y=...\n",
    "\n",
    "plt.scatter(X[:,interesting_columns[0]], X[:,interesting_columns[1]]\n",
    "            c = y #if I have the set of labels\n",
    "            #or set the same color\n",
    "            #, c='white'          # color filling the data markers\n",
    "            , edgecolors='black' # edge color for data markers\n",
    "            , marker='o'         # data marker shape, e.g. triangles (v<>^), square (s), star (*), ...\n",
    "            , s=50)              # data marker size\n",
    "plt.grid()  # plots a grid on the data\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMEans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find the optimal number of clusters\n",
    "# Prepare the results list that will contain pairs of\n",
    "# `inertia` and `silhouette_score` for each value of `k`, then, __for each value__ of `k`\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "k_range = list(range(2, 11))  # set the range of k values to test\n",
    "parameters_km = [{'n_clusters': k_range}]\n",
    "pg = list(ParameterGrid(parameters_km))\n",
    "\n",
    "inertias_km = []\n",
    "silhouette_scores_km = []\n",
    "for i in range(len(pg)):\n",
    "    km = KMeans(**(pg[i]), random_state=random_state)\n",
    "    y_km = km.fit_predict(X)\n",
    "    inertias_km.append(km.inertia_)\n",
    "    silhouette_scores_km.append(silhouette_score(X, y_km))\n",
    "\n",
    "\n",
    "# Plot inertia e siluettescore\n",
    "def two_plots(x, y1, y2, xlabel, y1label, y2label):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(y1label, color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    # we already handled the x-label with ax1\n",
    "    ax2.set_ylabel(y2label, color=color)\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_ylim(0, 1)  # the axis for silhouette is [0,1]\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "two_plots(x=k_range, y1=inertias_km, y2=silhouette_scores_km, xlabel='Number of clusters', y1label='Inertias', y2label='Silhouette scores'\n",
    "          )\n",
    "\n",
    "\n",
    "#Found the best parameter reinstanciate the estimator\n",
    "k_best = ...\n",
    "km = KMeans(n_clusters=k_best, \n",
    "            random_state=random_state)\n",
    "y_km = km.fit_predict(X)\n",
    "#Silhouette value over 0.5 is considered acceptable\n",
    "print(\"Number of clusters = {}\\t- Distortion = {:6.2f}\\t- Silhouette score = {:4.2f}\"\\\n",
    "    .format(k_best,inertias_km[k_range.index(k_best)],silhouette_scores_km[k_range.index(k_best)]))\n",
    "\n",
    "#Plot the pie to see how data have been clustered\n",
    "clust_sizes_km = np.unique(y_km,return_counts=True)\n",
    "pd.DataFrame(clust_sizes_km[1]).plot.pie(y=0, autopct='%1.1f%%', )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "parameters = [{'n_clusters': k_range\n",
    "                    , 'linkage' : ['ward', 'complete', 'average', 'single']}]\n",
    "pg = list(ParameterGrid(parameters))\n",
    "result_ac = []\n",
    "for i in range(len(pg)):\n",
    "    ac = AgglomerativeClustering(**(pg[i]))\n",
    "    y_ac = ac.fit_predict(X)\n",
    "    result_ac.append([pg[i]['linkage'],pg[i]['n_clusters'],silhouette_score(X,y_ac)])\n",
    "\n",
    "#create a table structured like that :   Linkage  n_cluster  silhouette_score\n",
    "df_result_ac = pd.DataFrame(data = result_ac, columns=['linkage','n_clusters','silhouette_score'])\n",
    "df_result_ac.sort_values(by='silhouette_score', ascending=False).head()\n",
    "\n",
    "#Add the line linkage_enc and find the best value of silhouette in the table considering the n_clusters nedeed store the index in a variable\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "df_result_ac['linkage_enc'] = oe.fit_transform(df_result_ac['linkage'].values.reshape(-1,1))\n",
    "df_result_ac.head()\n",
    "print(\"best parameter: \",pg[np.argmax(df_result_ac['silhouette_score'])] )\n",
    "index_best_score = ...\n",
    "\n",
    "#Reinstaciate the model\n",
    "ac = AgglomerativeClustering(**(pg[index_best_score]))\n",
    "y_ac = ac.fit_predict(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefult functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is useful to correct some misclassification especially of the noise because the noise will be mapped to the the most probable true_label\n",
    "# e.g if the majority of points labelled as noise (-1) correspond to a cluster 0 in the y_true, it means that we can convert\n",
    "# that noise into cluster 0 because apparently the real value is 0  but DBSCAN treats them like noise points.\n",
    "#The same accounts for the other label present in Y_pred. e.g if every time we have predicted 0 corresponds to a 1 apparently 1 is the way to do.\n",
    "\n",
    "\n",
    "def remap(y_true, y_pred):\n",
    "    y_mapped = y_pred.copy()\n",
    "    for lab in np.unique(y_pred):\n",
    "        true_l, count = np.unique(y_true[y_pred == lab], return_counts=True)\n",
    "        y_mapped[y_pred == lab] = true_l[np.argmax(count)]\n",
    "    return y_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a good starting approximation of min_points and eps use the elbow method as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all if we have a dataframe we transform it into a numpy array just for the sake of the following two cells\n",
    "X_ = df.to_numpy().copy()\n",
    "#For min_points we look around twice the number of attributes of the dataset (X.shape[1])\n",
    "min_points=2*X_.shape[1] \n",
    "#For eps we look at the elbow curve of the following graph and we range in its proximity \n",
    "k_distances = []\n",
    "for i in range(0, X_.shape[0]):\n",
    "    k_point_distances = []\n",
    "    for j in range(0, X_.shape[0]):\n",
    "        if i!=j:\n",
    "            dist = np.sqrt(sum((X_[i,:]-X_[j,:])**2))\n",
    "            k_point_distances.append(dist)\n",
    "    k_point_distances.sort()\n",
    "    k_distances.append(k_point_distances[min_points-1])\n",
    "    \n",
    "k_distances.sort(reverse=True)\n",
    "plt.plot(range(0,len(k_distances)), k_distances)\n",
    "plt.ylabel('eps')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Let's say that the elbow is present in the interval [0.9,1], try to look in an interval that includes it\n",
    "best_eps = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters using parameterGrid\n",
    "from sklearn.cluster import DBSCAN\n",
    "param_grid = {'eps': list(np.arange(0.01, 1, 0.01)), 'min_samples': list(range(1,10,1))}\n",
    "params = list(ParameterGrid(param_grid))\n",
    "\n",
    "# The unclustered score is a metric that measures the proportion of data points that are not assigned to any cluster.\n",
    "dbscan_out = pd.DataFrame(columns =  ['eps','min_samples','n_clusters','silhouette', 'unclust%'])\n",
    "fails = []\n",
    "for i in range(len(params)):\n",
    "    db = DBSCAN(**(params[i]))\n",
    "    y_db = db.fit_predict(X)\n",
    "    cluster_labels_all = np.unique(y_db)\n",
    "    cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "    n_clusters = len(cluster_labels)\n",
    "    if n_clusters > 1:\n",
    "        X_cl = X.iloc[y_db!=-1,:]\n",
    "        y_db_cl = y_db[y_db!=-1]\n",
    "        silhouette = silhouette_score(X_cl,y_db_cl)\n",
    "        uncl_p = (1 - y_db_cl.shape[0]/y_db.shape[0]) * 100\n",
    "        dbscan_out.loc[len(dbscan_out)] = [db.eps, db.min_samples, n_clusters, silhouette, uncl_p]\n",
    "\n",
    "sil_thr = 0.7  # visualize results only for combinations with silhouette above the threshold\n",
    "unc_thr = 10 # visualize results only for combinations with unclustered% below the threshold\n",
    "n_clu_max_thr = 4\n",
    "dbscan_out_trimmed = dbscan_out[(dbscan_out['silhouette']>=sil_thr)\\\n",
    "         & (dbscan_out['unclust%']<=unc_thr)\\\n",
    "         & (dbscan_out['n_clusters']<=n_clu_max_thr)]\n",
    "dbscan_out_trimmed\n",
    "\n",
    "\n",
    "#Once decided the best parameter reinstanciate the estimator\n",
    "best_eps = ...\n",
    "best_min_samples = ...\n",
    "\n",
    "db = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "y_db = db.fit_predict(X)\n",
    "cluster_labels_all = np.unique(y_db)\n",
    "cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "n_clusters = len(cluster_labels)\n",
    "print(\"There are {} clusters\".format(n_clusters))\n",
    "print(\"The cluster labels are {}\".format(cluster_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "interesting_columns = [0,1]\n",
    "\n",
    "def plot_clusters(X, y, dim, points,\n",
    "                  labels_prefix = 'cluster', \n",
    "                  points_name = 'centroids',\n",
    "                  colors = cm.tab10, # a qualitative map \n",
    "                      # https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "#                   colors = ['brown', 'orange', 'olive', \n",
    "#                             'green', 'cyan', 'blue', \n",
    "#                             'purple', 'pink'],\n",
    "#                   points_color = 'red'\n",
    "                  points_color = cm.tab10(10), # by default the last of the map (to be improved)\n",
    "                  title = None\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Plot a two dimensional projection of an array of labelled points\n",
    "    X:      array with at least two columns\n",
    "    y:      vector of labels, length as number of rows in X\n",
    "    dim:    the two columns to project, inside range of X columns, e.g. (0,1)\n",
    "    points: additional points to plot as 'stars'\n",
    "    labels_prefix: prefix to the labels for the legend ['cluster']\n",
    "    points_name:   legend name for the additional points ['centroids']\n",
    "    colors: a color map\n",
    "    points_color: the color for the points\n",
    "    \"\"\"\n",
    "    # plot the labelled (colored) dataset and the points\n",
    "    X_ = np.array(X).copy()\n",
    "    labels = np.unique(y)\n",
    "    for i in range(len(labels)):\n",
    "        color = colors(i / len(labels)) # choose a color from the map\n",
    "        plt.scatter(X_[y==labels[i],dim[0]], \n",
    "                    X_[y==labels[i],dim[1]], \n",
    "                    s=10, \n",
    "                    c = [color], # scatter requires a sequence of colors\n",
    "                    marker='s', \n",
    "                    label=labels_prefix+str(labels[i]))\n",
    "    plt.scatter(points[:,dim[0]], \n",
    "                points[:,dim[1]], \n",
    "                s=50, \n",
    "                marker='*', \n",
    "                c=[points_color], \n",
    "                label=points_name)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    if title!=None:plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#Call it\n",
    "cluster_labels_all = np.unique(y_km)\n",
    "cluster_labels = cluster_labels_all[cluster_labels_all != -1]\n",
    "n_clusters = len(cluster_labels)\n",
    "cluster_centers = np.empty(shape=(n_clusters,X.shape[1]))\n",
    "for i in cluster_labels:\n",
    "    cluster_centers[i,:] = np.mean(X.iloc[y_km==i,:], axis = 0)\n",
    "\n",
    "#Plot the clusters\n",
    "plot_clusters(X,y_db,dim=(interesting_columns[0],interesting_columns[1]), points = cluster_centers)\n",
    "#Without the centers\n",
    "plot_clusters(X,y_db,dim=(interesting_columns[0],interesting_columns[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARE CLUSTERINGS METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "#Rand index adjusted for chance.\n",
    "#The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "#The adjusted rand score being very low, means that the two clusterings are quite different.\n",
    "y_model1 = ...\n",
    "y_model2 = ...\n",
    "adjusted_rand_score(y_model1, y_model2)\n",
    "\n",
    "#Pair confusion matrix arising from two clusterings.\n",
    "#The pair confusion matrix \n",
    "#computes a 2 by 2 similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings.\n",
    "from sklearn.cluster import pair_confusion_matrix\n",
    "pair_confusion_matrix(y_model1, y_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54883a7121a713d77125f1e58771414b12d9415a58e298846e2d0c0165565864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
